{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgxomshQHAwO",
        "outputId": "8ddeb85c-014b-4477-d311-c11546a81527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'OnePixelAttackFashionMINST'...\n",
            "remote: Enumerating objects: 61, done.\u001b[K\n",
            "remote: Counting objects: 100% (61/61), done.\u001b[K\n",
            "remote: Compressing objects: 100% (47/47), done.\u001b[K\n",
            "remote: Total 61 (delta 24), reused 34 (delta 12), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (61/61), 31.00 MiB | 13.39 MiB/s, done.\n",
            "Resolving deltas: 100% (24/24), done.\n"
          ]
        }
      ],
      "source": [
        "# If running in Google Colab, import files\n",
        "try:\n",
        "    import google.colab\n",
        "    in_colab = True\n",
        "except:\n",
        "    in_colab = False\n",
        "\n",
        "if in_colab:\n",
        "    !git clone https://github.com/ahojszyk/OnePixelAttackFashionMINST.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrQd6i2yHSSF",
        "outputId": "af651858-a94b-44f3-bc77-6039da5c1da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler, TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, AveragePooling2D, GlobalAveragePooling2D, MaxPooling2D, Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import json\n",
        "import os\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "# Fashion MNIST class names\n",
        "class_names = [\n",
        "    \"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "    \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"\n",
        "]\n",
        "\n",
        "# Load Fashion MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "6vu3gQ3BN60u"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import keras\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import json\n",
        "import os\n",
        "import time\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsavyKqGHTT4"
      },
      "outputs": [],
      "source": [
        "class Net:\n",
        "\n",
        "    def __init__(self, epochs=200, batch_size=128, load_weights=True, net_type='LeNet'):\n",
        "\n",
        "        \"\"\"\n",
        "        A class representing various CNN architectures for CIFAR-10.\n",
        "        Includes training, evaluation, prediction, and visualization capabilities.\n",
        "        \"\"\"\n",
        "\n",
        "        self.name = net_type\n",
        "        self.model_filename =  f'OnePixelAttackFashionMINST/networks/models/{net_type}.keras'\n",
        "        self.checkpoint_filepath = f'OnePixelAttackFashionMINST/networks/models/{net_type}_checkpoint.keras'\n",
        "        self.num_classes = 10\n",
        "        self.input_shape = (28, 28, 1)  # Adjusted for Fashion MNIST\n",
        "        self.batch_size = batch_size\n",
        "        self.epochs = epochs\n",
        "        self.weight_decay = 0.0001\n",
        "        self.log_filepath = f'OnePixelAttackFashionMINST/networks/models/{net_type}'\n",
        "\n",
        "        if load_weights:\n",
        "            try:\n",
        "                self._model = load_model(self.model_filename)\n",
        "                print('Successfully loaded', self.name)\n",
        "            except (ImportError, ValueError, OSError):\n",
        "                print('Failed to load', self.name)\n",
        "\n",
        "    def build_model(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Build the model based on the specified network type.\n",
        "\n",
        "        Returns:\n",
        "            keras.Model: Compiled model.\n",
        "        \"\"\"\n",
        "\n",
        "        if self.name == 'LeNet':\n",
        "            model = Sequential()\n",
        "            model.add(Conv2D(6, kernel_size=5, strides=1, activation='relu', input_shape=self.input_shape, padding='same'))\n",
        "            model.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "            model.add(Conv2D(16, kernel_size=5, strides=1, activation='relu', padding='valid'))\n",
        "            model.add(AveragePooling2D(pool_size=2, strides=2))\n",
        "            model.add(Conv2D(120, kernel_size=5, strides=1, activation='relu', padding='valid'))\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(84, activation='relu'))\n",
        "            model.add(Dense(self.num_classes, activation='softmax'))\n",
        "\n",
        "        elif self.name == 'All Convolution':\n",
        "            model = Sequential()\n",
        "            model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', input_shape=self.input_shape))\n",
        "            model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same'))\n",
        "            model.add(Conv2D(96, (3, 3), activation='relu', padding = 'same', strides = 2))\n",
        "            model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
        "            model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same'))\n",
        "\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "            model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2))\n",
        "            model.add(Conv2D(192, (3, 3), activation='relu', padding = 'same', strides = 2))\n",
        "            model.add(Conv2D(192, (1, 1), activation='relu', padding='valid'))\n",
        "            model.add(Conv2D(10, (1, 1), activation='relu', padding='valid'))\n",
        "\n",
        "            model.add(GlobalAveragePooling2D())\n",
        "            model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "        elif self.name == 'VGG8':\n",
        "            model = Sequential()\n",
        "            model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
        "                            padding='same', input_shape=(28, 28, 1)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(MaxPooling2D((2, 2)))\n",
        "            model.add(Dropout(0.2))\n",
        "\n",
        "            model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(MaxPooling2D((2, 2)))\n",
        "            model.add(Dropout(0.3))\n",
        "\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Conv2D(512, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(MaxPooling2D((2, 2)))\n",
        "            model.add(Dropout(0.4))\n",
        "\n",
        "            model.add(Flatten())\n",
        "            model.add(Dense(8192, activation='relu', kernel_initializer='he_uniform'))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dense(1024, activation='relu', kernel_initializer='he_uniform'))\n",
        "            model.add(Dropout(0.5))\n",
        "            model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "        elif self.name == \"Network in Network\":\n",
        "            model = Sequential()\n",
        "            model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\", input_shape=self.input_shape))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Conv2D(160, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Conv2D(96, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "\n",
        "            model.add(Dropout(0.5))\n",
        "\n",
        "            model.add(Conv2D(192, (5, 5), padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Conv2D(192, (1, 1),padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(MaxPooling2D(pool_size=(3, 3),strides=(2,2),padding = 'same'))\n",
        "\n",
        "            model.add(Dropout(0.5))\n",
        "\n",
        "            model.add(Conv2D(192, (3, 3), padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Conv2D(192, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "            model.add(Conv2D(10, (1, 1), padding='same', kernel_regularizer=keras.regularizers.l2(self.weight_decay), kernel_initializer=\"he_normal\"))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Activation('relu'))\n",
        "\n",
        "            model.add(GlobalAveragePooling2D())\n",
        "            model.add(Activation('softmax'))\n",
        "\n",
        "        model.compile(optimizer=Adam(learning_rate=self.weight_decay), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def train(self, data_aug=True, early_stopping=True, patience=5):\n",
        "\n",
        "        \"\"\"\n",
        "        Train the model on the CIFAR-10 dataset.\n",
        "\n",
        "        Args:\n",
        "            data_aug (bool): Whether to use data augmentation.\n",
        "            early_stopping (bool): Whether to use early stopping.\n",
        "            patience (int): Number of epochs to wait for early stopping.\n",
        "        \"\"\"\n",
        "\n",
        "        # Load Fashion MNIST dataset\n",
        "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "        x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "        y_train = tf.keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = tf.keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "        # Build network\n",
        "        model = self.build_model()\n",
        "        model.summary()\n",
        "\n",
        "        print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "\n",
        "        # EarlyStopping callback\n",
        "        callbacks = []\n",
        "        if early_stopping:\n",
        "            early_stop = EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)\n",
        "            callbacks.append(early_stop)\n",
        "\n",
        "        # ModelCheckpoint callback\n",
        "        checkpoint_callback = ModelCheckpoint(\n",
        "            self.checkpoint_filepath,\n",
        "            monitor='val_loss',\n",
        "            save_best_only=True,\n",
        "            mode='min',\n",
        "            verbose=1\n",
        "        )\n",
        "        callbacks.append(checkpoint_callback)\n",
        "\n",
        "        # Start timing the training process\n",
        "        start_time = time.time()\n",
        "\n",
        "\n",
        "        # Using real-time data augmentation\n",
        "        if data_aug:\n",
        "            print('Using real-time data augmentation.')\n",
        "            datagen = ImageDataGenerator(\n",
        "                horizontal_flip=True,\n",
        "                width_shift_range=0.125,\n",
        "                height_shift_range=0.125,\n",
        "                fill_mode='nearest'\n",
        "            )\n",
        "            datagen.fit(x_train)\n",
        "\n",
        "            hist = model.fit(datagen.flow(x_train, y_train, batch_size=self.batch_size),\n",
        "                             epochs=self.epochs,\n",
        "                             validation_data=(x_test, y_test),\n",
        "                             callbacks=callbacks)\n",
        "        else:\n",
        "            print('No data augmentation.')\n",
        "            hist = model.fit(x_train, y_train, batch_size=self.batch_size,\n",
        "                             epochs=self.epochs,\n",
        "                             validation_data=(x_test, y_test),\n",
        "                             callbacks=callbacks)\n",
        "        # End timing\n",
        "        self.training_time = time.time() - start_time\n",
        "\n",
        "        # Save the training history\n",
        "        with open(f'{self.log_filepath}_training_history.json', 'w') as f:\n",
        "            json.dump(hist.history, f)\n",
        "\n",
        "        # Save the final model\n",
        "        model.save(self.model_filename)\n",
        "        self._model = model\n",
        "\n",
        "    def predict(self, img):\n",
        "        \"\"\"\n",
        "        Process the image and predict the labels in batch mode.\n",
        "        \"\"\"\n",
        "        img = np.expand_dims(img, axis=-1)\n",
        "        img = np.reshape(img, (-1, 28, 28, 1))\n",
        "\n",
        "        return self._model.predict(img, batch_size=self.batch_size, verbose=0)\n",
        "\n",
        "\n",
        "    def plot_history(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Plot training and validation loss and accuracy from training history.\n",
        "        \"\"\"\n",
        "\n",
        "        with open(f'{self.log_filepath}_training_history.json', 'r') as f:\n",
        "            history = json.load(f)\n",
        "\n",
        "        epochs = range(1, len(history['loss']) + 1)\n",
        "\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(epochs, history['loss'], 'bo-', label='Training Loss')\n",
        "        plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(epochs, history['accuracy'], 'bo-', label='Training Accuracy')\n",
        "        plt.plot(epochs, history['val_accuracy'], 'ro-', label='Validation Accuracy')\n",
        "        plt.title('Training and Validation Accuracy')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.suptitle(self.name)\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_confusion_matrix(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Generate and visualize the confusion matrix for the model predictions on test data.\n",
        "        \"\"\"\n",
        "\n",
        "        _, (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') # / 255\n",
        "        y_test = tf.keras.utils.to_categorical(y_test, self.num_classes)\n",
        "\n",
        "        y_pred = self._model.predict(x_test, batch_size=self.batch_size)\n",
        "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "        y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "        cm = confusion_matrix(y_true, y_pred_classes)\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
        "                    xticklabels=class_names, yticklabels=class_names)\n",
        "        plt.xlabel(\"Predicted Label\")\n",
        "        plt.ylabel(\"True Label\")\n",
        "        plt.title(f\"{self.name} Confusion Matrix\")\n",
        "        plt.show()\n",
        "\n",
        "    def visualize_predictions(self, num_samples=10):\n",
        "\n",
        "        \"\"\"\n",
        "        Visualize sample predictions alongside their true class labels.\n",
        "\n",
        "        Parameters:\n",
        "        - num_samples: Number of samples to visualize (default is 10).\n",
        "        \"\"\"\n",
        "\n",
        "        _, (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_test = x_test.reshape(-1, 28, 28, 1).astype('float32')# / 255\n",
        "        y_true = y_test.flatten()\n",
        "\n",
        "        indices = np.random.choice(range(len(x_test)), size=num_samples, replace=False)\n",
        "        fig, axs = plt.subplots(2, 5, figsize=(15, 7))\n",
        "        plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
        "\n",
        "        for i, idx in enumerate(indices):\n",
        "            img = x_test[idx].reshape(28, 28)\n",
        "            pred_label = np.argmax(self._model.predict(np.expand_dims(img, axis=0)))\n",
        "            true_class_name = class_names[y_true[idx]]\n",
        "            pred_class_name = class_names[pred_label]\n",
        "\n",
        "            ax = axs[i // 5, i % 5]\n",
        "            ax.imshow(img, cmap='gray')\n",
        "            ax.set_title(f\"True: {true_class_name}\\nPred: {pred_class_name}\",\n",
        "                         color=(\"green\" if true_class_name == pred_class_name else \"red\"))\n",
        "            ax.axis(\"off\")\n",
        "\n",
        "        plt.suptitle(\"Sample Predictions with True and Predicted Class Names\")\n",
        "        plt.show()\n",
        "\n",
        "    def predict_one(self, img):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the label for a single image.\n",
        "        \"\"\"\n",
        "        return self.predict(img)[0]\n",
        "\n",
        "    def count_params(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Return the number of trainable parameters in the model.\n",
        "\n",
        "        Returns:\n",
        "            int: Number of trainable parameters.\n",
        "        \"\"\"\n",
        "        return self._model.count_params()\n",
        "\n",
        "    def accuracy(self):\n",
        "\n",
        "        \"\"\"\n",
        "        Evaluate the model accuracy on the CIFAR-10 test dataset.\n",
        "        \"\"\"\n",
        "        (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "        x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "        x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "        y_train = tf.keras.utils.to_categorical(y_train, self.num_classes)\n",
        "        y_test = tf.keras.utils.to_categorical(y_test, self.num_classes)\n",
        "        x_train = x_train.astype('float32')\n",
        "        x_test = x_test.astype('float32')\n",
        "\n",
        "\n",
        "        return self._model.evaluate(x_test, y_test, verbose=0)[1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkOHglgzHWu9",
        "outputId": "90dc3842-35b6-4d47-c2bf-d84485d31211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded Network in Network\n",
            "Successfully loaded All Convolution\n",
            "Successfully loaded LeNet\n"
          ]
        }
      ],
      "source": [
        "# Load pretrained models\n",
        "NiN = Net(load_weights = True, net_type = 'Network in Network')\n",
        "AllConv = Net(load_weights = True, net_type = 'All Convolution')\n",
        "LeNet = Net(load_weights = True, net_type = 'LeNet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayW33X6UHml5"
      },
      "outputs": [],
      "source": [
        "models = [LeNet, AllConv, NiN]\n",
        "\n",
        "def assess_models(models_list):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of a list of models on the Fashion MNIST dataset and returns results as DataFrames.\n",
        "\n",
        "    Parameters:\n",
        "        models_list (list): A list of model objects. Each model should have:\n",
        "                            - `name` (str): The model's name.\n",
        "                            - `predict(data)` method: Returns predictions for the input data.\n",
        "                            - `count_params()` method: Returns the total number of parameters in the model.\n",
        "                            - `training_time` attribute: The time taken to train the model.\n",
        "\n",
        "    Returns:\n",
        "        tuple:\n",
        "            - model_performance_df (pd.DataFrame): A DataFrame with columns\n",
        "                                                  ['model_name', 'accuracy', 'parameter_count', 'training_time']\n",
        "            - correctly_classified_images_df (pd.DataFrame): A DataFrame with columns\n",
        "                                                            ['model_name', 'image_index', 'true_label', 'confidence', 'predictions']\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize lists to store results\n",
        "    correctly_classified_images = []\n",
        "    model_performance = []\n",
        "\n",
        "    # Load Fashion MNIST dataset\n",
        "    (train_data, train_labels), (test_data, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "    # Reshape test data to include a channel dimension\n",
        "    test_data = test_data.reshape(-1, 28, 28, 1)\n",
        "\n",
        "    # Iterate through each model in the list\n",
        "    for model in models_list:\n",
        "        # Train the model (if needed)\n",
        "        if not hasattr(model, '_model') or model._model is None:\n",
        "            model.train()  # Ensure the model is trained\n",
        "\n",
        "        # Get predictions for the test dataset\n",
        "        predictions = model.predict(test_data)\n",
        "\n",
        "        # Identify correctly classified images\n",
        "        correctly_classified = [\n",
        "            [model.name, img_idx, true_label, np.max(pred), pred]\n",
        "            for img_idx, (true_label, pred) in enumerate(zip(test_labels, predictions))\n",
        "            if true_label == np.argmax(pred)  # Check if prediction matches the true label\n",
        "        ]\n",
        "\n",
        "        # Calculate accuracy as the ratio of correctly classified images to total images\n",
        "        accuracy = len(correctly_classified) / len(test_data)\n",
        "\n",
        "        # Add correctly classified image details and model performance stats\n",
        "        correctly_classified_images += correctly_classified\n",
        "        model_performance.append([model.name, accuracy, model.count_params(), model.training_time])\n",
        "\n",
        "    # Convert lists to DataFrames\n",
        "    correctly_classified_images_df = pd.DataFrame(\n",
        "        correctly_classified_images,\n",
        "        columns=['model_name', 'image_index', 'true_label', 'confidence', 'predictions']\n",
        "    )\n",
        "    model_performance_df = pd.DataFrame(\n",
        "        model_performance,\n",
        "        columns=['model_name', 'accuracy', 'parameter_count', 'training_time']\n",
        "    )\n",
        "\n",
        "    # Return DataFrames\n",
        "    return model_performance_df, correctly_classified_images_df\n",
        "\n",
        "\n",
        "network_stats, correct_imgs = assess_models(models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4Mi6PaeNIRD"
      },
      "outputs": [],
      "source": [
        "from OnePixelAttackFashionMINST.DifferentialEvolution import differential_evolution\n",
        "\n",
        "class Attack:\n",
        "    def __init__(self, x_test, y_test, models, csv_path, correct_imgs, maxiter=75, popsize=400, verbose=False):\n",
        "    \"\"\"\n",
        "    Initializes the Attack class with data, models, and parameters.\n",
        "\n",
        "    Parameters:\n",
        "        x_test (numpy.ndarray): The test dataset images to be used in the attack.\n",
        "        y_test (numpy.ndarray): The test dataset labels corresponding to the images.\n",
        "        models (list): A list of model objects to be attacked.\n",
        "        csv_path (str): The path to the results CSV file where attack results will be saved.\n",
        "        correct_imgs (pd.DataFrame): A DataFrame containing the images that were correctly classified by the models.\n",
        "        maxiter (int): The maximum number of iterations for the differential evolution algorithm.\n",
        "        popsize (int): The population size used in the differential evolution algorithm.\n",
        "        verbose (bool): Flag to enable or disable verbose logging during the attack process.\n",
        "    \"\"\"\n",
        "        self.x_test = x_test\n",
        "        self.y_test = y_test\n",
        "        self.models = models\n",
        "        self.csv_path = csv_path\n",
        "        self.correct_imgs = correct_imgs\n",
        "        self.maxiter = maxiter\n",
        "        self.popsize = popsize\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Ensure the results CSV exists or create an empty DataFrame\n",
        "        if os.path.exists(csv_path):\n",
        "            # Load the DataFrame and ensure headers are present\n",
        "            self.results_df = pd.read_csv(csv_path)\n",
        "            expected_columns = [\n",
        "                'model_name', 'pixel_count', 'img_id', 'actual_class',\n",
        "                'predicted_class', 'success', 'confidence_diff',\n",
        "                'prior_probs', 'predicted_probs', 'perturbation', 'target_class'\n",
        "            ]\n",
        "            if list(self.results_df.columns) != expected_columns:\n",
        "                raise ValueError(f\"The CSV at {csv_path} does not have the expected headers.\")\n",
        "        else:\n",
        "            # Create a new DataFrame with the required columns\n",
        "            columns = [\n",
        "                'model_name', 'pixel_count', 'img_id', 'actual_class',\n",
        "                'predicted_class', 'success', 'confidence_diff',\n",
        "                'prior_probs', 'predicted_probs', 'perturbation', 'target_class'\n",
        "            ]\n",
        "            self.results_df = pd.DataFrame(columns=columns)\n",
        "\n",
        "        # Track completed image-model-pixel combinations\n",
        "        self.completed_combinations = set(zip(\n",
        "            self.results_df['model_name'],\n",
        "            self.results_df['pixel_count'],\n",
        "            self.results_df['img_id']\n",
        "        ))\n",
        "\n",
        "    def _get_sampled_images(self, samples):\n",
        "        \"\"\"\n",
        "        Samples 10% of images per class to ensure a balanced dataset.\n",
        "\n",
        "        Parameters:\n",
        "            samples (int): Total number of images to sample from the dataset.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array of sampled image indices.\n",
        "        \"\"\"\n",
        "        # Get the list of fully correct images (images correctly classified by all models)\n",
        "        total_models = self.correct_imgs['model_name'].nunique()\n",
        "        fully_correct_imgs = self.correct_imgs.groupby('image_index').filter(\n",
        "            lambda x: x['model_name'].nunique() == total_models\n",
        "        )\n",
        "\n",
        "        # Exclude already used images\n",
        "        existing_imgs = self.results_df['img_id'].unique() if not self.results_df.empty else []\n",
        "        eligible_images = fully_correct_imgs[~fully_correct_imgs['image_index'].isin(existing_imgs)]\n",
        "\n",
        "        # Ensure there are enough images left to sample from\n",
        "        if len(eligible_images) < samples:\n",
        "            raise ValueError(f\"Not enough unique images remaining. Needed: {samples}, Available: {len(eligible_images)}\")\n",
        "\n",
        "        # Perform balanced sampling across classes\n",
        "        sampled_images = eligible_images.groupby('true_label').apply(\n",
        "            lambda x: x.sample(int(samples / 10), replace=False)\n",
        "        )\n",
        "        return sampled_images['image_index'].unique()\n",
        "\n",
        "    @staticmethod\n",
        "    def perturb_image(xs, img):\n",
        "        \"\"\"\n",
        "        Perturbs an image using the given pixel perturbations.\n",
        "\n",
        "        Parameters:\n",
        "            xs (numpy.ndarray): Array of perturbations to apply to the image.\n",
        "            img (numpy.ndarray): The original image to perturb.\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Array of perturbed images after applying the perturbations.\n",
        "        \"\"\"\n",
        "        if xs.ndim < 2:\n",
        "            xs = np.array([xs])\n",
        "\n",
        "        img = img.reshape(28, 28)\n",
        "        tile = [len(xs)] + [1] * img.ndim\n",
        "        imgs = np.tile(img, tile)\n",
        "        xs = xs.astype(int)\n",
        "\n",
        "        for x, img in zip(xs, imgs):\n",
        "            pixels = np.split(x, len(x) // 3)\n",
        "            for pixel in pixels:\n",
        "                x_pos, y_pos, grayscale_value = pixel\n",
        "                x_pos = np.clip(x_pos, 0, 27)\n",
        "                y_pos = np.clip(y_pos, 0, 27)\n",
        "                img[x_pos, y_pos] = grayscale_value\n",
        "\n",
        "        return imgs\n",
        "\n",
        "    def predict_classes(self, xs, img, target_class, model, minimize=True):\n",
        "        \"\"\"\n",
        "        Predicts the class probabilities of perturbed images.\n",
        "\n",
        "        Parameters:\n",
        "            xs (numpy.ndarray): Array of perturbations to apply to the image.\n",
        "            img (numpy.ndarray): The original image to perturb.\n",
        "            target_class (int): The target class for the attack.\n",
        "            model (object): The model used to evaluate the perturbed image.\n",
        "            minimize (bool): Whether to minimize or maximize the objective (target class).\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Predicted probabilities for the target class after perturbing the image.\n",
        "        \"\"\"\n",
        "        imgs_perturbed = self.perturb_image(xs, img)\n",
        "        predictions = model.predict(imgs_perturbed)[:, target_class]\n",
        "        return predictions if minimize else 1 - predictions\n",
        "\n",
        "    def attack_success(self, x, img, target_class, model, targeted_attack):\n",
        "        \"\"\"\n",
        "        Checks if the attack is successful.\n",
        "\n",
        "        Parameters:\n",
        "            x (numpy.ndarray): Perturbation vector applied to the image.\n",
        "            img (numpy.ndarray): The original image.\n",
        "            target_class (int): The target class for the attack.\n",
        "            model (object): The model used to evaluate the perturbed image.\n",
        "            targeted_attack (bool): Whether this is a targeted attack.\n",
        "\n",
        "        Returns:\n",
        "            bool: True if the attack successfully manipulates the model’s prediction, False otherwise.\n",
        "        \"\"\"\n",
        "        attack_image = self.perturb_image(x, img)\n",
        "        confidence = model.predict(attack_image)[0]\n",
        "        predicted_class = np.argmax(confidence)\n",
        "\n",
        "        if ((targeted_attack and predicted_class == target_class) or\n",
        "            (not targeted_attack and predicted_class != target_class)):\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def attack(self, img_id, model, target=None, pixel_count=1):\n",
        "        \"\"\"\n",
        "        Executes a one-pixel attack on a single image.\n",
        "\n",
        "        Parameters:\n",
        "            img_id (int): ID of the image to attack.\n",
        "            model (object): The model used for the attack.\n",
        "            target (int): Target class for the attack (used in targeted attacks).\n",
        "            pixel_count (int): Number of pixels to perturb in the image.\n",
        "\n",
        "        Returns:\n",
        "            dict: Attack results including metadata and model predictions after the attack.\n",
        "        \"\"\"\n",
        "        targeted_attack = target is not None\n",
        "        target_class = target if targeted_attack else self.y_test[img_id]\n",
        "        bounds = [(0, 28), (0, 28), (0, 255)] * pixel_count\n",
        "        popmul = max(1, self.popsize // len(bounds))\n",
        "\n",
        "        def predict_fn(xs):\n",
        "            return self.predict_classes(xs, self.x_test[img_id], target_class, model, target is None)\n",
        "\n",
        "        def callback_fn(x, convergence):\n",
        "            return self.attack_success(x, self.x_test[img_id], target_class, model, targeted_attack)\n",
        "\n",
        "        attack_result = differential_evolution(\n",
        "            predict_fn, bounds, maxiter=self.maxiter, popsize=popmul,\n",
        "            recombination=1, atol=-1, callback=callback_fn, polish=False)\n",
        "\n",
        "        attack_image = self.perturb_image(attack_result.x, self.x_test[img_id])[0]\n",
        "        prior_probs = model.predict_one(self.x_test[img_id])\n",
        "        predicted_probs = model.predict_one(attack_image)\n",
        "        predicted_class = np.argmax(predicted_probs)\n",
        "        actual_class = self.y_test[img_id]\n",
        "        success = predicted_class != actual_class\n",
        "        cdiff = prior_probs[actual_class] - predicted_probs[actual_class]\n",
        "\n",
        "        if self.verbose:\n",
        "            plt.imshow(attack_image, cmap='gray')\n",
        "            print(f\"Actual: {actual_class}, Predicted: {predicted_class}\")\n",
        "\n",
        "        # Add target_class to the returned result\n",
        "        return [\n",
        "            model.name, pixel_count, img_id, actual_class, predicted_class,\n",
        "            success, cdiff, prior_probs, predicted_probs, attack_result.x, target_class\n",
        "        ]\n",
        "\n",
        "    def attack_all(self, samples=100, pixels=(1, 3, 5), targeted=False):\n",
        "        \"\"\"\n",
        "        Executes one-pixel attacks on multiple images and saves results.\n",
        "\n",
        "        Parameters:\n",
        "            samples (int): Number of images to sample for the attack.\n",
        "            pixels (tuple): Tuple of pixel counts to use for the attacks.\n",
        "            targeted (bool): Flag indicating whether to perform targeted attacks.\n",
        "\n",
        "        Returns:\n",
        "            pd.DataFrame: DataFrame containing the results of the attacks, including predictions and metadata.\n",
        "        \"\"\"\n",
        "        sampled_images = self._get_sampled_images(samples)\n",
        "\n",
        "        for model in self.models:\n",
        "            for pixel_count in pixels:\n",
        "                for img_id in sampled_images:\n",
        "                    if (model.name, pixel_count, img_id) in self.completed_combinations:\n",
        "                        continue\n",
        "\n",
        "                    print(f'\\n{model.name} - image {img_id} - pixel count {pixel_count}')\n",
        "                    targets = [None] if not targeted else range(10)\n",
        "\n",
        "                    for target in targets:\n",
        "                        if targeted and target == self.y_test[img_id]:\n",
        "                            continue\n",
        "\n",
        "                        result = self.attack(img_id, model, target, pixel_count)\n",
        "                        result_df = pd.DataFrame([result], columns=self.results_df.columns)\n",
        "                        result_df.to_csv(self.csv_path, mode='a', header=False, index=False)\n",
        "                        self.completed_combinations.add((model.name, pixel_count, img_id))\n",
        "\n",
        "        final_results_df = pd.read_csv(self.csv_path)\n",
        "        return final_results_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pdw-ZDPjZAU4",
        "outputId": "4616daf7-37e3-448a-f010-e28f2c684586"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-28-b2817a7d5f89>:79: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  sampled_images = eligible_images.groupby('true_label').apply(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LeNet - image 6339 - pixel count 2\n",
            "Actual: 0, Predicted: 0\n",
            "\n",
            "LeNet - image 7055 - pixel count 2\n",
            "Actual: 1, Predicted: 1\n",
            "\n",
            "LeNet - image 8198 - pixel count 2\n",
            "Actual: 2, Predicted: 2\n",
            "\n",
            "LeNet - image 931 - pixel count 2\n",
            "Actual: 3, Predicted: 3\n",
            "\n",
            "LeNet - image 6131 - pixel count 2\n",
            "Actual: 4, Predicted: 4\n",
            "\n",
            "LeNet - image 4210 - pixel count 2\n",
            "Actual: 5, Predicted: 5\n",
            "\n",
            "LeNet - image 4453 - pixel count 2\n",
            "Actual: 6, Predicted: 0\n",
            "\n",
            "LeNet - image 365 - pixel count 2\n",
            "Actual: 7, Predicted: 7\n",
            "\n",
            "LeNet - image 4851 - pixel count 2\n",
            "Actual: 8, Predicted: 7\n",
            "\n",
            "LeNet - image 3023 - pixel count 2\n",
            "Actual: 9, Predicted: 5\n",
            "\n",
            "LeNet - image 6339 - pixel count 4\n",
            "Actual: 0, Predicted: 6\n",
            "\n",
            "LeNet - image 7055 - pixel count 4\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-15a933e94031>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m final_results = attack_instance.attack_all(\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0msamples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mpixels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b2817a7d5f89>\u001b[0m in \u001b[0;36mattack_all\u001b[0;34m(self, samples, pixels, targeted)\u001b[0m\n\u001b[1;32m    211\u001b[0m                             \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m                         \u001b[0mresult_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                         \u001b[0mresult_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'a'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b2817a7d5f89>\u001b[0m in \u001b[0;36mattack\u001b[0;34m(self, img_id, model, target, pixel_count)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattack_success\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargeted_attack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         attack_result = differential_evolution(\n\u001b[0m\u001b[1;32m    168\u001b[0m             \u001b[0mpredict_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpopsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpopmul\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             recombination=1, atol=-1, callback=callback_fn, polish=False)\n",
            "\u001b[0;32m/content/OnePixelAttackFashionMINST/DifferentialEvolution.py\u001b[0m in \u001b[0;36mdifferential_evolution\u001b[0;34m(func, bounds, args, strategy, maxiter, popsize, tol, mutation, recombination, seed, callback, disp, polish, init, atol)\u001b[0m\n\u001b[1;32m    212\u001b[0m                                          \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                                          disp=disp, init=init, atol=atol)\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msolve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OnePixelAttackFashionMINST/DifferentialEvolution.py\u001b[0m in \u001b[0;36msolve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    567\u001b[0m             \u001b[0;31m# evolve the population by a generation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m                 \u001b[0mwarning_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/OnePixelAttackFashionMINST/DifferentialEvolution.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_constraint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scale_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m         \u001b[0menergies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nfev\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mitersize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b2817a7d5f89>\u001b[0m in \u001b[0;36mpredict_fn\u001b[0;34m(xs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mpredict_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcallback_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvergence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-b2817a7d5f89>\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, xs, img, target_class, model, minimize)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \"\"\"\n\u001b[1;32m    122\u001b[0m         \u001b[0mimgs_perturbed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperturb_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs_perturbed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mminimize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-f03d9cd709ca>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure the shape is (1, 28, 28, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    448\u001b[0m     ):\n\u001b[1;32m    449\u001b[0m         \u001b[0;31m# Create an iterator that yields batches of input data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m         epoch_iterator = TFEpochIterator(\n\u001b[0m\u001b[1;32m    451\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_distribute_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDistributedDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m             dataset = self._distribute_strategy.experimental_distribute_dataset(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tf_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0menumerate_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mget_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0moptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/array_data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrab_one\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m             dataset = dataset.map(\n\u001b[0m\u001b[1;32m    216\u001b[0m                 \u001b[0mgrab_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_parallel_calls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTOTUNE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m   2309\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2310\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmap_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2311\u001b[0;31m     return map_op._map_v2(\n\u001b[0m\u001b[1;32m   2312\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2313\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m_map_v2\u001b[0;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[1;32m     38\u001b[0m         input_dataset, map_func, preserve_cardinality=True, name=name)\n\u001b[1;32m     39\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     return _ParallelMapDataset(\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/map_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m     self._map_func = structured_function.StructuredFunctionWrapper(\n\u001b[0m\u001b[1;32m    149\u001b[0m         \u001b[0mmap_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/structured_function.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 265\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    266\u001b[0m     \u001b[0;31m# There is no graph to add in eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[0;34m&=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_concrete_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m     \u001b[0;31m# Implements PolymorphicFunction.get_concrete_function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m     \u001b[0mconcrete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_garbage_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m     \u001b[0mconcrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1219\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m         \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1221\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize_uninitialized_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    694\u001b[0m     )\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# Force the definition of the function for these arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m     self._concrete_variable_creation_fn = tracing_compilation.trace_function(\n\u001b[0m\u001b[1;32m    697\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     concrete_function = _maybe_define_function(\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracing_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    248\u001b[0m   \u001b[0;31m# Use a timer for graph building only if not already inside a function. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;31m# avoids double counting graph building time for nested functions.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m   with monitoring.MonitoredTimer(\n\u001b[0m\u001b[1;32m    251\u001b[0m       \u001b[0m_graph_building_time_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m   ) if not ops.inside_function() else contextlib.nullcontext():\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Load Fashion-MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "x_test = x_test.reshape(-1, 28, 28, 1)  # Reshape for compatibility with models\n",
        "\n",
        "\n",
        "# Results CSV path\n",
        "csv_path = \"resultsFashionMNIST.csv\"\n",
        "\n",
        "# Create an instance of the Attack class\n",
        "attack_instance = Attack(\n",
        "    x_test=x_test,\n",
        "    y_test=y_test,\n",
        "    models=models,\n",
        "    csv_path=csv_path,\n",
        "    correct_imgs=correct_imgs,\n",
        "    maxiter=75,\n",
        "    popsize=400,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "final_results = attack_instance.attack_all(\n",
        "    samples=10,\n",
        "    pixels=(2, 4),\n",
        "    targeted=False\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}